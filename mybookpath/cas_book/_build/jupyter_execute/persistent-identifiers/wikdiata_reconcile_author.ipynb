{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with data that did not come from a bibliographic source you likely only have a name in natural order and maybe some other biographical information. For example you may have a list of names and a couple of other data points like a university they attended or what country they are from. While Wikidata is not a bibliographic centric system there is a lot of data and works in the ecosystem. In this tutorial we are going to use Wikidata to reconcile names and try to get associated work titles.\n",
    "\n",
    "\n",
    "Getting started checklist:\n",
    "1. A CSV or TSV file with metadata, at minimum it needs to contain the authorâ€™s full name and the full title of the work.\n",
    "3. Python, with Pandas, Requests module installed and internet connection\n",
    "\n",
    "In this example we are comparing data of people that have won literary awards, it also has some other data like universities attended and some dates. Depending on the dataset you are using you will have different data points to use to compare but the approach is the same. You can an example of the data we are using below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>given_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>elite_institution</th>\n",
       "      <th>graduate_degree</th>\n",
       "      <th>mfa_degree</th>\n",
       "      <th>iowa_mfa_person_id</th>\n",
       "      <th>stegner</th>\n",
       "      <th>role</th>\n",
       "      <th>prize_institution</th>\n",
       "      <th>prize_name</th>\n",
       "      <th>prize_year</th>\n",
       "      <th>prize_genre</th>\n",
       "      <th>prize_type</th>\n",
       "      <th>prize_amount</th>\n",
       "      <th>title_of_winning_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1105</td>\n",
       "      <td>John Barkham</td>\n",
       "      <td>John</td>\n",
       "      <td>Barkham</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>1961</td>\n",
       "      <td>prose</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>To Kill A Mockingbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1789</td>\n",
       "      <td>Paul Harding</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Harding</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduate</td>\n",
       "      <td>University of Iowa</td>\n",
       "      <td>2455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>winner</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>2010</td>\n",
       "      <td>prose</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Tinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1789</td>\n",
       "      <td>Paul Harding</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Harding</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduate</td>\n",
       "      <td>University of Iowa</td>\n",
       "      <td>2455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>winner</td>\n",
       "      <td>PEN America</td>\n",
       "      <td>Robert W. Bingham Prize for Debut Short Story ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>prose</td>\n",
       "      <td>book</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>Tinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>522</td>\n",
       "      <td>David Kennedy</td>\n",
       "      <td>David</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>male</td>\n",
       "      <td>Stanford University, Yale University</td>\n",
       "      <td>graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>2010</td>\n",
       "      <td>prose</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Tinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>354</td>\n",
       "      <td>Charles Johnson</td>\n",
       "      <td>Charles</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>2010</td>\n",
       "      <td>prose</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Tinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>70</td>\n",
       "      <td>Alfred Kreynborg</td>\n",
       "      <td>Alfred</td>\n",
       "      <td>Kreynborg</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>1961</td>\n",
       "      <td>poetry</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Times Three: Selected Verse From Three Decades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1430</td>\n",
       "      <td>Louis Untermeyer</td>\n",
       "      <td>Louis</td>\n",
       "      <td>Untermeyer</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>1961</td>\n",
       "      <td>poetry</td>\n",
       "      <td>book</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Times Three: Selected Verse From Three Decades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>545</td>\n",
       "      <td>David St John</td>\n",
       "      <td>David</td>\n",
       "      <td>St John</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduate</td>\n",
       "      <td>University of Iowa</td>\n",
       "      <td>3483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>National Book Foundation</td>\n",
       "      <td>National Book Award</td>\n",
       "      <td>2007</td>\n",
       "      <td>poetry</td>\n",
       "      <td>book</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Time And Materials: Poems, 1997-2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2305</td>\n",
       "      <td>Vijay Seshadri</td>\n",
       "      <td>Vijay</td>\n",
       "      <td>Seshadri</td>\n",
       "      <td>male</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>graduate</td>\n",
       "      <td>Columbia University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judge</td>\n",
       "      <td>National Book Foundation</td>\n",
       "      <td>National Book Award</td>\n",
       "      <td>2007</td>\n",
       "      <td>poetry</td>\n",
       "      <td>book</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Time And Materials: Poems, 1997-2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1962</td>\n",
       "      <td>Robert Hass</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Hass</td>\n",
       "      <td>male</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>winner</td>\n",
       "      <td>National Book Foundation</td>\n",
       "      <td>National Book Award</td>\n",
       "      <td>2007</td>\n",
       "      <td>poetry</td>\n",
       "      <td>book</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Time And Materials: Poems, 1997-2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person_id         full_name given_name   last_name gender  \\\n",
       "150       1105      John Barkham       John     Barkham   male   \n",
       "151       1789      Paul Harding       Paul     Harding   male   \n",
       "152       1789      Paul Harding       Paul     Harding   male   \n",
       "153        522     David Kennedy      David     Kennedy   male   \n",
       "154        354   Charles Johnson    Charles     Johnson   male   \n",
       "155         70  Alfred Kreynborg     Alfred   Kreynborg   male   \n",
       "156       1430  Louis Untermeyer      Louis  Untermeyer   male   \n",
       "157        545     David St John      David     St John   male   \n",
       "158       2305    Vijay Seshadri      Vijay    Seshadri   male   \n",
       "159       1962       Robert Hass     Robert        Hass   male   \n",
       "\n",
       "                        elite_institution graduate_degree  \\\n",
       "150                                   NaN             NaN   \n",
       "151                                   NaN        graduate   \n",
       "152                                   NaN        graduate   \n",
       "153  Stanford University, Yale University        graduate   \n",
       "154                                   NaN        graduate   \n",
       "155                                   NaN             NaN   \n",
       "156                                   NaN             NaN   \n",
       "157                                   NaN        graduate   \n",
       "158                   Columbia University        graduate   \n",
       "159                   Stanford University        graduate   \n",
       "\n",
       "              mfa_degree iowa_mfa_person_id stegner    role  \\\n",
       "150                  NaN                NaN     NaN   judge   \n",
       "151   University of Iowa               2455     NaN  winner   \n",
       "152   University of Iowa               2455     NaN  winner   \n",
       "153                  NaN                NaN     NaN   judge   \n",
       "154                  NaN                NaN     NaN   judge   \n",
       "155                  NaN                NaN     NaN   judge   \n",
       "156                  NaN                NaN     NaN   judge   \n",
       "157   University of Iowa               3483     NaN   judge   \n",
       "158  Columbia University                NaN     NaN   judge   \n",
       "159                  NaN                NaN     NaN  winner   \n",
       "\n",
       "            prize_institution  \\\n",
       "150       Columbia University   \n",
       "151       Columbia University   \n",
       "152               PEN America   \n",
       "153       Columbia University   \n",
       "154       Columbia University   \n",
       "155       Columbia University   \n",
       "156       Columbia University   \n",
       "157  National Book Foundation   \n",
       "158  National Book Foundation   \n",
       "159  National Book Foundation   \n",
       "\n",
       "                                            prize_name  prize_year  \\\n",
       "150                                     Pulitzer Prize        1961   \n",
       "151                                     Pulitzer Prize        2010   \n",
       "152  Robert W. Bingham Prize for Debut Short Story ...        2010   \n",
       "153                                     Pulitzer Prize        2010   \n",
       "154                                     Pulitzer Prize        2010   \n",
       "155                                     Pulitzer Prize        1961   \n",
       "156                                     Pulitzer Prize        1961   \n",
       "157                                National Book Award        2007   \n",
       "158                                National Book Award        2007   \n",
       "159                                National Book Award        2007   \n",
       "\n",
       "    prize_genre prize_type  prize_amount  \\\n",
       "150       prose       book       15000.0   \n",
       "151       prose       book       15000.0   \n",
       "152       prose       book       25000.0   \n",
       "153       prose       book       15000.0   \n",
       "154       prose       book       15000.0   \n",
       "155      poetry       book       15000.0   \n",
       "156      poetry       book       15000.0   \n",
       "157      poetry       book       10000.0   \n",
       "158      poetry       book       10000.0   \n",
       "159      poetry       book       10000.0   \n",
       "\n",
       "                              title_of_winning_book  \n",
       "150                           To Kill A Mockingbird  \n",
       "151                                         Tinkers  \n",
       "152                                         Tinkers  \n",
       "153                                         Tinkers  \n",
       "154                                         Tinkers  \n",
       "155  Times Three: Selected Verse From Three Decades  \n",
       "156  Times Three: Selected Verse From Three Decades  \n",
       "157            Time And Materials: Poems, 1997-2005  \n",
       "158            Time And Materials: Poems, 1997-2005  \n",
       "159            Time And Materials: Poems, 1997-2005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first steps will be defining some variables we will be using, set these variables below based on your setup:\n",
    "\n",
    "\n",
    "`path_to_tsv` - the path to the TSV/CSV file you want to run it on\n",
    "\n",
    "`id_author_name` - the name of the column header in the file that contains author's name\n",
    "\n",
    "`id_title_name` - the name of the column header in the file that contains title of the work\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call, if you want the script to run slower\n",
    "\n",
    "We also will load the modules we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/path/to/the_file.tsv\"\n",
    "id_author_name = 'author'\n",
    "id_title_name = 'title'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with the file we need to define some helper functions. The first function (`add_qid`) will be what each row of the file is run throuh to modify the values. We are also define `normalize_string` and `levenshtein` to compare strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qid(d):\n",
    "\n",
    "\n",
    "    # if there is already a value skip it\n",
    "    if 'author_wikidata' in d:\n",
    "        if type(d['author_wikidata']) == str:        \n",
    "            # print('Skip',d[id_column_name])\n",
    "            return d\n",
    "\n",
    "    \n",
    "    # We are going to search Wikdiata for the author name first using this `query` endpoint, we pass the string and it will \n",
    "    # return Items in the system by label\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        'action':'query',\n",
    "        'srsearch':d[id_column_name],\n",
    "        'format':'json',\n",
    "        'list':'search',\n",
    "        'srlimit':'10'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    # make a list of the qids to check\n",
    "    qids = []\n",
    "    for s in data['query']['search']:\n",
    "        qids.append(s['title'])\n",
    "\n",
    "    total_hits = data['query']['searchinfo']['totalhits']\n",
    "\n",
    "    # no hits, change nothing\n",
    "    if len(qids) == 0:\n",
    "        return d\n",
    "\n",
    "    # we now have a bunch of Items that have matched the string, we are going to ask \n",
    "    # wikdiata to return some other data we can use to compare \n",
    "    # this part of the script really depends on what data you have to compare to,\n",
    "    # below is an example of having a decent amount of additional information to use to \n",
    "    # compare with, you maynot have some of this data of course and would not use those sections\n",
    "    # build the SPARQL query we are going to use\n",
    "    qids_with_quotes = []\n",
    "    for q in qids:\n",
    "        qids_with_quotes.append(f'wd:{q}')\n",
    "\n",
    "    # this sparql query is being built to ask for all these possible data points that we have in our dataset to compare with\n",
    "    # you can look at items in wikidat to see what properties are available\n",
    "    sparql = f\"\"\"\n",
    "        SELECT ?item ?itemLabel ?occupation ?occupationLabel ?birth ?death ?award ?awardLabel ?viaf ?lccn ?education ?educationLabel\n",
    "        WHERE \n",
    "        {{\n",
    "\n",
    "            VALUES ?item {{ { \" \".join(qids_with_quotes)  }}}\n",
    "\n",
    "            ?item wdt:P31 wd:Q5.\n",
    "          \n",
    "            optional{{\n",
    "              ?item wdt:P106 ?occupation.\n",
    "            }}\n",
    "            optional{{\n",
    "              ?item wdt:P569 ?birth.\n",
    "            }}\n",
    "            optional{{\n",
    "              ?item wdt:P570 ?death.\n",
    "            }}          \n",
    "            optional{{\n",
    "              ?item wdt:P166 ?award.\n",
    "            }}    \n",
    "            optional{{\n",
    "              ?item wdt:P214 ?viaf.\n",
    "            }}              \n",
    "            optional{{\n",
    "              ?item wdt:P244 ?lccn.\n",
    "            }}\n",
    "            optional{{\n",
    "              ?item wdt:P69 ?education.\n",
    "            }}\n",
    "\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'query' : sparql\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    \n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    people = {}\n",
    "\n",
    "    # did we get any results\n",
    "    if len(data['results']['bindings']) > 0:\n",
    "\n",
    "      # build out our people dictonary with the data we have recived\n",
    "      # we are going to use a score system where when data matches we increase the score and the \n",
    "      # wikidata item with the highest score will be our match\n",
    "\n",
    "      for result in data['results']['bindings']:\n",
    "\n",
    "        qid = result['item']['value'].split('/')[-1]\n",
    "\n",
    "        if qid not in people:\n",
    "\n",
    "          people[qid] = {\n",
    "            'qid':qid,\n",
    "            'score':0,\n",
    "            'score_log':[],\n",
    "            'occupation':[],\n",
    "            'birth':None,\n",
    "            'death':None,\n",
    "            'viaf':None,\n",
    "            'lccn':None,\n",
    "            'label':None,\n",
    "            'award': [],\n",
    "            'education':[]\n",
    "          }\n",
    "        \n",
    "        # make string lists out of these catagories\n",
    "        if 'occupationLabel' in result:\n",
    "          people[qid]['occupation'].append(result['occupationLabel']['value'])\n",
    "          people[qid]['occupation'] = list(set(people[qid]['occupation']))\n",
    "\n",
    "        if 'awardLabel' in result:\n",
    "          people[qid]['award'].append(result['awardLabel']['value'])\n",
    "          people[qid]['award'] = list(set(people[qid]['award']))\n",
    "\n",
    "        if 'educationLabel' in result:\n",
    "          people[qid]['education'].append(result['educationLabel']['value'])\n",
    "          people[qid]['education'] = list(set(people[qid]['education']))\n",
    "\n",
    "\n",
    "        if 'itemLabel' in result:\n",
    "          people[qid]['label'] = result['itemLabel']['value']\n",
    "\n",
    "        if 'birth' in result:\n",
    "          try:            \n",
    "            # if it is set to this value it is \"20.th century, which is unhelpful in our use\"\n",
    "            if result['birth']['value'] != '2000-01-01T00:00:00Z':\n",
    "              people[qid]['birth'] = int(result['birth']['value'].split('-')[0])\n",
    "          except:\n",
    "            people[qid]['birth'] = None\n",
    "\n",
    "        if 'death' in result:\n",
    "          try:\n",
    "            if result['death']['value'] != '2000-01-01T00:00:00Z':\n",
    "              people[qid]['death'] = int(result['death']['value'].split('-')[0])\n",
    "          except:\n",
    "            people[qid]['death'] = None\n",
    "\n",
    "        if 'viaf' in result:\n",
    "          people[qid]['viaf'] = result['viaf']['value']\n",
    "\n",
    "        if 'lccn' in result:\n",
    "          people[qid]['lccn'] = result['lccn']['value']\n",
    "\n",
    "\n",
    "    # You can setup different types of checks, \n",
    "    # for example in our data set we are looking at writes, so if they do not have a \"writerly\" occupation\n",
    "    # then we can exclude them by default\n",
    "    for p in people:\n",
    "\n",
    "      has_writerly_occ = False\n",
    "      \n",
    "      for occ in ['writer', 'poet', 'novelist', 'short story writer', 'author', 'literary critic', 'journalist', 'biographer', 'historian', 'comics artist', 'playwright']:\n",
    "        if occ in people[p]['occupation']:\n",
    "          has_writerly_occ = True\n",
    "      \n",
    "      if has_writerly_occ == False:\n",
    "        people[p]['score'] = -1\n",
    "        people[p]['score_log'].append(\"No writerly occ\")\n",
    "        continue\n",
    "      else:\n",
    "        people[p]['score'] = people[p]['score'] +  1\n",
    "        people[p]['score_log'].append(\"Has writerly occ\")\n",
    "      \n",
    "\n",
    "      # Here is one option to check the name to see how similar it is to the name you have\n",
    "      # using levenshtein distance to compare\n",
    "      if levenshtein(normalize_string(people[p]['label']),normalize_string(d['full_name'])) < 3:\n",
    "        people[p]['score'] = people[p]['score'] + 1\n",
    "        people[p]['score_log'].append(\"Has very similar name\")\n",
    "      elif levenshtein(normalize_string(people[p]['label']),normalize_string(d['full_name'])) >= 3:\n",
    "        people[p]['score'] = people[p]['score'] - 1\n",
    "      elif levenshtein(normalize_string(people[p]['label']),normalize_string(d['full_name'])) >= 5:\n",
    "        people[p]['score'] = people[p]['score'] - 2\n",
    "      elif levenshtein(normalize_string(people[p]['label']),normalize_string(d['full_name'])) >= 8:\n",
    "        people[p]['score'] = people[p]['score'] - 3        \n",
    "      \n",
    "\n",
    "      # points for having a VIAF or LCCN\n",
    "      # in this dataset we add extra points for records that have LCCN or VIAF values \n",
    "      # which points to that they are the bibilograhic person we want\n",
    "\n",
    "      if people[p]['lccn'] != None or people[p]['viaf'] != None:\n",
    "        people[p]['score'] = people[p]['score'] + 1\n",
    "        people[p]['score_log'].append(\"Has VIAF OR LCCN\")\n",
    "      \n",
    "      # points for being being alive for important date in our data set\n",
    "      if people[p]['death'] != None:\n",
    "        try:\n",
    "          # there is a try here for bad data in d['prize_year'], if the int(d['prize_year']) fails just skip it\n",
    "          award_year = int(d['prize_year'])\n",
    "          if award_year <= people[p]['death']:\n",
    "            people[p]['score'] = people[p]['score'] + 1     \n",
    "            people[p]['score_log'].append(\"not dead when awarded\")\n",
    "        except:\n",
    "          pass\n",
    "      \n",
    "      # if they were not born when the award was granted then very bad\n",
    "      if people[p]['birth'] != None:\n",
    "        try:          \n",
    "          award_year = int(d['prize_year'])\n",
    "          if award_year <= people[p]['birth']:\n",
    "            people[p]['score'] = -1\n",
    "            people[p]['score_log'].append(\"not alive when awarded = -1\")        \n",
    "        except:\n",
    "          pass\n",
    "      \n",
    "      # compare awards won compared to our award won\n",
    "      if len(people[p]['award']) >0 and type(d['prize_name']) == str:\n",
    "        awards = \" \".join(people[p]['award']).lower().replace('fellowship','').replace('prize','').replace('award','')\n",
    "        if d['prize_name'].lower().replace('fellowship','').replace('prize','').replace('award','') in awards:\n",
    "          people[p]['score'] = people[p]['score'] + 1\n",
    "          people[p]['score_log'].append(\"matched on award\")        \n",
    "\n",
    "      # compare education\n",
    "      if len(people[p]['education']) > 0  != None and type(d['elite_institution']) == str:\n",
    "        edu = \" \".join(people[p]['education']).lower().replace('university of ','').replace('university','').replace('college','')\n",
    "        if d['elite_institution'].lower().replace('university of ','').replace('university','').replace('college','') in edu:\n",
    "          people[p]['score'] = people[p]['score'] + 1\n",
    "          people[p]['score_log'].append(\"matched on edu\")        \n",
    "\n",
    "      if len(people[p]['education']) > 0  != None and type(d['mfa_degree']) == str:\n",
    "        edu = \" \".join(people[p]['education']).lower().replace('university of ','').replace('university','').replace('college','')\n",
    "        if d['mfa_degree'].lower().replace('university of ','').replace('university','').replace('college','') in edu:\n",
    "          people[p]['score'] = people[p]['score'] + 1\n",
    "          people[p]['score_log'].append(\"matched on edu\")  \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    best_person_score = 0\n",
    "    best_person = None\n",
    "    for p in people:\n",
    "      if people[p]['score'] > best_person_score:\n",
    "        best_person_score = people[p]['score']\n",
    "        best_person = people[p]\n",
    "\n",
    "    if best_person != None:\n",
    "      if best_person_score > 1:\n",
    "\n",
    "        d['author_wikidata'] = best_person['qid']\n",
    "        d['author_wikidata_label'] = best_person['label']\n",
    "\n",
    "        if best_person['viaf'] != None:\n",
    "          d['author_viaf'] = best_person['viaf']\n",
    "        if best_person['lccn'] != None:\n",
    "          d['author_lccn'] = best_person['lccn']\n",
    "\n",
    "        # some debug fields, the score they got and the log of activity\n",
    "        d['match_score'] = best_person_score\n",
    "        d['match_log'] = \"|\".join(best_person['score_log'])\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.lower()\n",
    "    s = s.casefold()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = s.replace('the','')\n",
    "    return s\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to load the Pandas module and load the data we are using, you can adjust the `sep` argument to change what delimiter is being used (for example if you are using a CSV file, change it to \",\"). Once loaded we pass each record to the `add_qid()` function to kick off adding the data to the record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/the_file.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path_to_tsv, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m d: add_qid(d),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/the_file.tsv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df = df.apply(lambda d: add_qid(d),axis=1 )  \n",
    "\n",
    "# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\n",
    "df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the block above but it breaks the CSV/TSV into multiple chunks and writes it out after each chunk, this allows for recovery from any errors such as as internet timeout or other problems that would cause you to loose all progress unless the script runs flawlessly, you would likely want to use this approch for larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_qid(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the Qid for the author you can ask wikidata for the Titles that author has created using a SPARQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(author_qid):\n",
    "\n",
    "    sparql = f\"\"\"\n",
    "        SELECT ?item ?itemLabel \n",
    "        WHERE \n",
    "        {{\n",
    "        ?item wdt:P50 wd:{{author_qid}}. #P50 is has author\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}       \n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'query' : sparql\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "\n",
    "    # did we get any results\n",
    "    if len(data['results']['bindings']) > 0:\n",
    "\n",
    "      for result in data['results']['bindings']:\n",
    "        title_qid = result['item']['value'].split('/')[-1]\n",
    "        title_label = result['itemLabel']['value']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}