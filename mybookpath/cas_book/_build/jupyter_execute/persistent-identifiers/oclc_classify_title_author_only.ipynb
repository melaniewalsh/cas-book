{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCLC Classify Title Author\n",
    "\n",
    "In this tutorial we will be using OCLC Classify service to reconcile bibliographic titles and authors. In this situation we only have the title of the work and the author’s name. Our goal is to enrich the dataset with unique persistent identifiers for the names and titles provided. The approach we are taking in getting these identifiers is to start with the Work, which is the highest level of hierarchy. You can imagine a Work as having many instances, or editions that belong to the same work. So in order to get those instance level information we need to reconcile the Work to the OCLC Classify Work. \n",
    "\n",
    "\n",
    "Getting started checklist:\n",
    "1. A CSV or TSV file with metadata, at minimum it needs to contain the author’s full name and the full title of the work.\n",
    "2. A OCLC WSkey for the Classify service, this can be generated if you have a subscription/membership with OCLC. You need to speak with someone at your institution who has access to your organization’s account at http://platform.worldcat.org/wskey/\n",
    "3. Python, with Pandas, Requests and BeautifulSoup module installed and internet connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first steps will be defining some variables we will be using, set these variables below based on your setup:\n",
    "\n",
    "\n",
    "`path_to_tsv` - the path to the TSV/CSV file you want to run it on\n",
    "\n",
    "`id_author_name` - the name of the column header in the file that contains author's name\n",
    "\n",
    "`id_title_name` - the name of the column header in the file that contains title of the work\n",
    "\n",
    "`id_author_viaf` - the name of the column header that the author's viaf number will be put into\n",
    "\n",
    "`id_author_authorized_heading` - the name of the column header that the  author's authorized heading string\n",
    "\n",
    "`id_author_lccn`- the name of the column header that the author's lccn number\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call, if you want the script to run slower\n",
    "\n",
    "`WSKey` - the OCLC WSkey, will be a 80 character alpha numeric code\n",
    "\n",
    "We also will load the modules we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/path/to/the_file.tsv\"\n",
    "id_author_name = 'author'\n",
    "id_title_name = 'title'\n",
    "id_author_viaf  = 'author_viaf'\n",
    "id_author_authorized_heading  = 'author_authorized_heading'\n",
    "id_author_lccn = 'author_lccn'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "WSkey = \"WSKeyXXXXXXXXXXXXXX\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of what data you run through this script would look like, here are a few rows from a dataset that would be used with this approch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>total_weeks</th>\n",
       "      <th>first_week</th>\n",
       "      <th>debut_rank</th>\n",
       "      <th>best_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1132</td>\n",
       "      <td>COMMONWEALTH</td>\n",
       "      <td>Ann Patchett</td>\n",
       "      <td>2016</td>\n",
       "      <td>18</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1133</td>\n",
       "      <td>COMPANY MAN</td>\n",
       "      <td>Joseph Finder</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>2005-05-08</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1134</td>\n",
       "      <td>COMPULSION</td>\n",
       "      <td>Jonathan Kellerman</td>\n",
       "      <td>2008</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-04-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1135</td>\n",
       "      <td>COMPULSION</td>\n",
       "      <td>Meyer Levin</td>\n",
       "      <td>1956</td>\n",
       "      <td>54</td>\n",
       "      <td>1956-11-18</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1136</td>\n",
       "      <td>CONCEALED IN DEATH</td>\n",
       "      <td>J. D. Robb</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1137</td>\n",
       "      <td>CONDOMINIUM</td>\n",
       "      <td>John D. MacDonald</td>\n",
       "      <td>1977</td>\n",
       "      <td>27</td>\n",
       "      <td>1977-04-24</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1138</td>\n",
       "      <td>CONFESSIONAL</td>\n",
       "      <td>Jack Higgins</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>1985-07-07</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1139</td>\n",
       "      <td>CONFESSIONS OF A SOCIOPATHIC SOCIAL CLIMBER</td>\n",
       "      <td>Adèle Lang</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-06-23</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>114</td>\n",
       "      <td>A GARDEN TO THE EASTWARD</td>\n",
       "      <td>Harold Lamb</td>\n",
       "      <td>1947</td>\n",
       "      <td>2</td>\n",
       "      <td>1947-04-13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1140</td>\n",
       "      <td>CONFESSIONS OF A WILD CHILD</td>\n",
       "      <td>Jackie Collins</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-02-23</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                        title              author  \\\n",
       "150  1132                                 COMMONWEALTH        Ann Patchett   \n",
       "151  1133                                  COMPANY MAN       Joseph Finder   \n",
       "152  1134                                   COMPULSION  Jonathan Kellerman   \n",
       "153  1135                                   COMPULSION         Meyer Levin   \n",
       "154  1136                           CONCEALED IN DEATH          J. D. Robb   \n",
       "155  1137                                  CONDOMINIUM   John D. MacDonald   \n",
       "156  1138                                 CONFESSIONAL        Jack Higgins   \n",
       "157  1139  CONFESSIONS OF A SOCIOPATHIC SOCIAL CLIMBER          Adèle Lang   \n",
       "158   114                     A GARDEN TO THE EASTWARD         Harold Lamb   \n",
       "159  1140                  CONFESSIONS OF A WILD CHILD      Jackie Collins   \n",
       "\n",
       "     year  total_weeks  first_week  debut_rank  best_rank  \n",
       "150  2016           18  2016-10-02           1          1  \n",
       "151  2005            1  2005-05-08          15         15  \n",
       "152  2008            5  2008-04-13           1          1  \n",
       "153  1956           54  1956-11-18          10          2  \n",
       "154  2014            4  2014-03-09           1          1  \n",
       "155  1977           27  1977-04-24           9          3  \n",
       "156  1985            1  1985-07-07          15         15  \n",
       "157  2002            1  2002-06-23          14         14  \n",
       "158  1947            2  1947-04-13          13         13  \n",
       "159  2014            1  2014-02-23          13         13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with the file we need to define two helper functions. The first function (`add_classify`) will be what each row of the file is run throuh to modify the values. The second helper function (`extract_classify`) is to parse the results returned from the Classify service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classify(d):\n",
    "\n",
    "    # You can add some logic here to skip rows that already have some data if you previously ran it\n",
    "    # if 'oclc_classify' in d:\n",
    "    #     if type(d['oclc_classify']) == str:        \n",
    "    #         print('Skip',d[id_author_name])\n",
    "    #         return d\n",
    "\n",
    "\n",
    "    # make the call out to the classift service\n",
    "    headers = {'X-OCLC-API-Key': WSkey}\n",
    "    params = {'author': d[id_author_name], 'title': d[id_title_name], 'summary' : 'false', 'maxRecs':100}\n",
    "    r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "    work_parsed = None\n",
    "    work_unparsed = None\n",
    "\n",
    "    # different response codes mean different things:\n",
    "    # 0:\tSuccess. Single-work summary response provided.\n",
    "    # 2:\tSuccess. Single-work detail response provided.\n",
    "    # 4:\tSuccess. Multi-work response provided.\n",
    "    # 100:\tNo input. The method requires an input argument.\n",
    "    # 101:\tInvalid input. The standard number argument is invalid.\n",
    "    # 102:\tNot found. No data found for the input argument.\n",
    "    # 200:\tUnexpected error.\n",
    "\n",
    "    if r.text.find('<response code=\"2\"/>') > -1:\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"4\"/>') > -1:\n",
    "        \n",
    "        # we need to look through this reponse since it returned multiple possiblities, we will just be selecting the one with the most holdings\n",
    "        soup = BeautifulSoup(str(r.text))\n",
    "        work_soup = soup.find(\"works\")\n",
    "        largest_count = 0\n",
    "        largest_work = None\n",
    "        for work in work_soup.find_all(\"work\"):\n",
    "            if int(work['holdings']) > largest_count:\n",
    "                largest_count = int(work['holdings'])\n",
    "                largest_work = work\n",
    "\n",
    "        # once we have that one we want to use, make the request again with its ID now\n",
    "        params = {'owi': largest_work['owi'], 'summary' : 'false', 'maxRecs':100}\n",
    "        r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"100\"/>') > -1 or r.text.find('<response code=\\\\\"100\\\\\"/>') >-1:\n",
    "        print(params,'100: No input. The method requires an input argument.')\n",
    "    elif r.text.find('<response code=\"101\"/>') > -1 or r.text.find('<response code=\\\\\"101\\\\\"/>') >-1:\n",
    "        print(params,'101: Invalid input. The standard number argument is invalid.')\n",
    "    elif r.text.find('<response code=\"102\"/>') > -1 or r.text.find('<response code=\\\\\"102\\\\\"/>') >-1:\n",
    "        print(params,'102: Not found. No data found for the input argument.')\n",
    "    elif r.text.find('<response code=\"200\"/>') > -1 or r.text.find('<response code=\\\\\"200\\\\\"/>') >-1:\n",
    "        print(params,'200: Unexpected error.')\n",
    "    else:\n",
    "        print(\"unknown Problem:\",r.text)\n",
    "\n",
    "\n",
    "    if work_parsed != None:\n",
    "        # we are going to store the raw XML from the response in the file as well as the extracted information\n",
    "        d['oclc_classify'] = work_unparsed\n",
    "\n",
    "        # check if the columns we want to add are not yet there in the row, if not add them as null\n",
    "        if id_author_viaf not in d:\n",
    "            d[id_author_viaf] = None\n",
    "\n",
    "        if id_author_authorized_heading not in d:\n",
    "            d[id_author_authorized_heading] = None\n",
    "\n",
    "        if id_author_lccn not in d:\n",
    "            d[id_author_lccn] = None\n",
    "\n",
    "        # add in the author info if is missing\n",
    "        if pd.isnull(d[id_author_viaf]) == True and work_parsed['work_author'] != None:\n",
    "            if work_parsed['work_author']['viaf'] != None:\n",
    "                d[id_author_authorized_heading] = work_parsed['work_author']['name']\n",
    "                d[id_author_viaf] = work_parsed['work_author']['viaf']\n",
    "        \n",
    "        if pd.isnull(d[id_author_lccn]) == True and work_parsed['work_author'] != None:\n",
    "            if work_parsed['work_author']['lccn'] != None:                    \n",
    "                d[id_author_lccn] = work_parsed['work_author']['lccn']\n",
    "                d[id_author_authorized_heading] = work_parsed['work_author']['name']\n",
    "\n",
    "        d['oclc_eholdings'] = work_parsed['work_eholdings']\n",
    "        d['oclc_holdings'] = work_parsed['work_holdings']\n",
    "        d['oclc_owi'] = work_parsed['work_owi']\n",
    "\n",
    "\n",
    "    # if we need to script to run slower we can configure it setting the  pause_between_req variable above \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second helper function, the Classify service returns a XML blob with data that can be parsed. We use the BeautifulSoup module to help read a parse the data.\n",
    "The function returns a dictonary with differnt keys:\n",
    "\n",
    "`work_statement_responsibility` - the string statement of responsibility\n",
    "\n",
    "`work_editions` - the total number of editions this work has\n",
    "\n",
    "`work_eholdings` - the total eletronic holdings this work has\n",
    "\n",
    "`work_format` - what format the work is, likely \"Book\"\n",
    "\n",
    "`work_holdings` - how many instances of this work exists in all the OCLC membership libraries\n",
    "\n",
    "`work_itemtype` - likely \"itemtype-book\"\n",
    "\n",
    "`work_owi` - a Work identifier from OCLC, mostly only used inside this Classify service\n",
    "\n",
    "`work_title` - title of the work\n",
    "\n",
    "`authors` - A list of dictonaries that contain `name` `lccn` `viaf` for each contributor, their authorized heading and identfiiers\n",
    "\n",
    "`work_author` - the \"main\" contributor, like the author opposed to illustrator or other contributor, a way to differentiate the main author from other contributors\n",
    "\n",
    "`normalized_ddc` - the most common dewey decimal value for this work\n",
    "\n",
    "`normalized_lcc` - the most common library of congress classifciation number for this work\n",
    "\n",
    "`editions` - a list of dictonaries for all the editions (instances) that represent this work in OCLC insitutions, each dict contains: `author` `eholdings` `format` `holdings` `itemtype` `language` `oclc` `title`\n",
    "\n",
    "You can view the Classify documentation for more information on the fields returned: https://classify.oclc.org/classify2/api_docs/classify.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify(xml):\n",
    "\n",
    "\t\t# we are parsing the XML with BeautifulSoup to make it easier to query\n",
    "\t\tsoup = BeautifulSoup(str(xml))\n",
    "\n",
    "\t\t# search for the <work> element\n",
    "\t\twork_soup = soup.find(\"work\")\n",
    "\n",
    "\t\tif work_soup == None:\n",
    "\t\t\t# print(\"can not parse xml:\")\n",
    "\t\t\t# print(xml)\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tresults = {}\n",
    "\n",
    "\t\t# each one of these are attributes on the <work> element, if they are present add them otherwise set it to None/null\n",
    "\t\tresults['work_statement_responsibility'] = None if work_soup.has_attr('author') == False else work_soup['author']\n",
    "\t\tresults['work_editions'] = None if work_soup.has_attr('editions') == False else int(work_soup['editions'])\n",
    "\t\tresults['work_eholdings'] = None if work_soup.has_attr('eholdings') == False else int(work_soup['eholdings'])\n",
    "\t\tresults['work_format'] = None if work_soup.has_attr('format') == False else work_soup['format']\n",
    "\t\tresults['work_holdings'] = None if work_soup.has_attr('holdings') == False else int(work_soup['holdings'])\n",
    "\t\tresults['work_itemtype'] = None if work_soup.has_attr('itemtype') == False else work_soup['itemtype']\n",
    "\t\tresults['work_owi'] = None if work_soup.has_attr('owi') == False else work_soup['owi']\n",
    "\t\tresults['work_title'] = None if work_soup.has_attr('title') == False else work_soup['title']\n",
    "\t\tresults['main_oclc'] = work_soup.text\n",
    "\n",
    "\t\t# the authors nested in their own elemnts, so find all of them\n",
    "\t\tauthors_soup = soup.find_all(\"author\")\n",
    "\t\tresults['authors'] = []\n",
    "\t\t# find the attributres for each authors and save these pieces of info for each one\n",
    "\t\tfor a in authors_soup:\n",
    "\t\t\tresults['authors'].append({\n",
    "\t\t\t\t\t\"name\" : a.text.split('[')[0].strip(),\n",
    "\t\t\t\t\t\"lccn\" : None if a.has_attr('lc') == False else a['lc'],\n",
    "\t\t\t\t\t\"viaf\" : None if a.has_attr('viaf') == False else a['viaf']\n",
    "\t\t\t\t})\n",
    "\t\t# clean up them if they were \"null\" strings in the data\n",
    "\t\tfor a in results['authors']:\n",
    "\t\t\tif a['lccn'] == 'null':\n",
    "\t\t\t\ta['lccn'] = None\t\n",
    "\t\t\tif a['viaf'] == 'null':\n",
    "\t\t\t\ta['viaf'] = None\t\n",
    "\n",
    "\t\t# try to find the first main contributor\n",
    "\t\t# the main contributor is usally the first in the work_statement_responsibility, but is just a string\n",
    "\t\t# so try to match that name to the <authors> element to get their other information and mark them as being the\n",
    "\t\t# \"main\" contributor\n",
    "\t\tresults['work_author'] = None\n",
    "\t\tif results['work_statement_responsibility'] != None:\n",
    "\t\t\tif len(results['work_statement_responsibility'].split(\"|\"))>0:\n",
    "\t\t\t\tfirst_author = results['work_statement_responsibility'].split(\"|\")[0].strip()\n",
    "\t\t\t\tfor a in results['authors']:\n",
    "\t\t\t\t\tprint(a['name'].split('[')[0].strip(), first_author )\n",
    "\t\t\t\t\tif a['name'].strip() == first_author:\n",
    "\t\t\t\t\t\tresults['work_author'] = a\n",
    "\n",
    "\n",
    "\t\t# no classifications by default and populate if they exist later\n",
    "\t\tresults[\"normalized_ddc\"] = None\n",
    "\t\tresults[\"normalized_lcc\"] = None\n",
    "\n",
    "\t\t# both Dewey and Library of Congress Classification are added, these are the most common \n",
    "\t\t# out of all of the holdings in OCLC\n",
    "\t\tddc_soup = soup.find(\"ddc\")\n",
    "\t\tif ddc_soup != None:\n",
    "\t\t\tddc_soup = soup.find(\"ddc\").find(\"mostpopular\")\n",
    "\t\t\tif ddc_soup != None:\n",
    "\t\t\t\tif ddc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_ddc\"] = ddc_soup['nsfa']\n",
    "\n",
    "\t\tlcc_soup = soup.find(\"lcc\")\n",
    "\t\tif lcc_soup != None:\n",
    "\t\t\tlcc_soup = soup.find(\"lcc\").find(\"mostpopular\")\n",
    "\t\t\tif lcc_soup != None:\n",
    "\t\t\t\tif lcc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_lcc\"] = lcc_soup['nsfa']\n",
    "\n",
    "\t\t# headings are subject headings, and are in FAST headings\n",
    "\t\tresults[\"headings\"] = []\n",
    "\t\theading_soup = soup.find_all(\"heading\")\n",
    "\t\tfor h in heading_soup:\n",
    "\t\t\tresults[\"headings\"].append({\n",
    "\t\t\t\t\t\"id\" : h['ident'],\n",
    "\t\t\t\t\t\"src\": h['src'],\n",
    "\t\t\t\t\t\"value\" : h.text\n",
    "\t\t\t\t})\n",
    "\t\t\t\n",
    "\t\t# number of editions \n",
    "\t\tedition_soup = soup.find_all(\"edition\")\n",
    "\t\t# print(isbn,len(edition_soup))\n",
    "\t\tresults[\"editions\"] = []\n",
    "\t\t# for each \"edition\" pull out the info for it and store it\n",
    "\t\tfor e in edition_soup:\n",
    "\t\t\tedition = {}\n",
    "\t\t\tedition['author'] = None if e.has_attr('author') == False else e['author']\n",
    "\t\t\tedition['eholdings'] = None if e.has_attr('eholdings') == False else int(e['eholdings'])\n",
    "\t\t\tedition['format'] = None if e.has_attr('format') == False else e['format']\n",
    "\t\t\tedition['holdings'] = None if e.has_attr('holdings') == False else int(e['holdings'])\n",
    "\t\t\tedition['itemtype'] = None if e.has_attr('itemtype') == False else e['itemtype']\n",
    "\t\t\tedition['language'] = None if e.has_attr('language') == False else e['language']\n",
    "\t\t\tedition['oclc'] = None if e.has_attr('oclc') == False else e['oclc']\n",
    "\t\t\tedition['title'] = None if e.has_attr('title') == False else e['title']\n",
    "\t\t\tresults[\"editions\"].append(edition)\n",
    "\t\t\t\n",
    "\t\t# the first one is always the one with the largest holdings so mark that one as largest\n",
    "\t\tif len(results[\"editions\"]) > 0:\n",
    "\t\t\tresults[\"largest_holding_oclc\"] = results[\"editions\"][0]['oclc']\n",
    "\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to load the Pandas module and load the data we are using, you can adjust the `sep` argument to change what delimiter is being used (for example if you are using a CSV file, change it to \",\"). Once loaded we pass each record to the `add_classify()` function to kick off adding the data to the record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/the_file.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path_to_tsv, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m d: add_classify(d),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/the_file.tsv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df = df.apply(lambda d: add_classify(d),axis=1 )  \n",
    "\n",
    "# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\n",
    "df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the block above but it breaks the CSV/TSV into multiple chunks and writes it out after each chunk, this allows for recovery from any errors such as as internet timeout or other problems that would cause you to loose all progress unless the script runs flawlessly, you would likely want to use this approch for larger datasets. Also uncomment the starting check in add_classify:\n",
    "```\n",
    "def add_classify(d):\n",
    "\n",
    "    # You can add some logic here to skip rows that already have some data if you previously ran it\n",
    "    # if 'oclc_classify' in d:\n",
    "    #     if type(d['oclc_classify']) == str:        \n",
    "    #         print('Skip',d[id_author_name])\n",
    "    #         return d\n",
    "\n",
    "```\n",
    "\n",
    "To allow it to check if it needs to skip the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_classify(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the data through this process you would have a few new fields added to the dataset the most important being the authorized heading, LCCN, VIAF and some information about the Work level. Now that we have some base identifiers in the dataset we can use other [scripts found here](https://github.com/Post45-Data-Collective/data-utilities/tree/main/enrichment) that use this same approch to add even more identifiers for the author and Work/Instance. Here is an example output of running this data set through all of these scripts:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>author_authorized_heading</th>\n",
       "      <th>author_lccn</th>\n",
       "      <th>author_viaf</th>\n",
       "      <th>author_wikidata</th>\n",
       "      <th>best_rank</th>\n",
       "      <th>debut_rank</th>\n",
       "      <th>first_week</th>\n",
       "      <th>oclc</th>\n",
       "      <th>oclc_eholdings</th>\n",
       "      <th>oclc_holdings</th>\n",
       "      <th>oclc_isbn</th>\n",
       "      <th>oclc_owi</th>\n",
       "      <th>title</th>\n",
       "      <th>total_weeks</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1132</td>\n",
       "      <td>Ann Patchett</td>\n",
       "      <td>Patchett, Ann</td>\n",
       "      <td>n91108359</td>\n",
       "      <td>54362733</td>\n",
       "      <td>Q433485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>932576291.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>3917.0</td>\n",
       "      <td>9780062491794</td>\n",
       "      <td>2.840265e+09</td>\n",
       "      <td>COMMONWEALTH</td>\n",
       "      <td>18</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1133</td>\n",
       "      <td>Joseph Finder</td>\n",
       "      <td>Finder, Joseph,</td>\n",
       "      <td>n82134077</td>\n",
       "      <td>36971667</td>\n",
       "      <td>Q2125259</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2005-05-08</td>\n",
       "      <td>57352767.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>9780312319168</td>\n",
       "      <td>2.011108e+06</td>\n",
       "      <td>COMPANY MAN</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1134</td>\n",
       "      <td>Jonathan Kellerman</td>\n",
       "      <td>Kellerman, Jonathan</td>\n",
       "      <td>n79116770</td>\n",
       "      <td>2559502</td>\n",
       "      <td>Q1349245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-04-13</td>\n",
       "      <td>166358565.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3919.0</td>\n",
       "      <td>9780345465276</td>\n",
       "      <td>1.128836e+08</td>\n",
       "      <td>COMPULSION</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1135</td>\n",
       "      <td>Meyer Levin</td>\n",
       "      <td>Levin, Meyer, 1905-1981</td>\n",
       "      <td>n79055883</td>\n",
       "      <td>27067610</td>\n",
       "      <td>Q966340</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1956-11-18</td>\n",
       "      <td>288021.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>0786703199</td>\n",
       "      <td>5.817719e+07</td>\n",
       "      <td>COMPULSION</td>\n",
       "      <td>54</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1136</td>\n",
       "      <td>J. D. Robb</td>\n",
       "      <td>Robb, J. D., 1950-</td>\n",
       "      <td>n95081025</td>\n",
       "      <td>43552467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>849719027.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>3602.0</td>\n",
       "      <td>9780399164439</td>\n",
       "      <td>1.744466e+09</td>\n",
       "      <td>CONCEALED IN DEATH</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1137</td>\n",
       "      <td>John D. MacDonald</td>\n",
       "      <td>MacDonald, John D. (John Dann), 1916-1986</td>\n",
       "      <td>n79045202</td>\n",
       "      <td>7393665</td>\n",
       "      <td>Q558425</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1977-04-24</td>\n",
       "      <td>2767566.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0397012039</td>\n",
       "      <td>1.965416e+08</td>\n",
       "      <td>CONDOMINIUM</td>\n",
       "      <td>27</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1138</td>\n",
       "      <td>Jack Higgins</td>\n",
       "      <td>Higgins, Jack, 1929-</td>\n",
       "      <td>n79059853</td>\n",
       "      <td>39378467</td>\n",
       "      <td>Q433708</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1985-07-07</td>\n",
       "      <td>11650359.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2384.0</td>\n",
       "      <td>0812830253</td>\n",
       "      <td>4.233915e+06</td>\n",
       "      <td>CONFESSIONAL</td>\n",
       "      <td>1</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1139</td>\n",
       "      <td>Adèle Lang</td>\n",
       "      <td>Lang, Adèle</td>\n",
       "      <td>n98880006</td>\n",
       "      <td>10699038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>2002-06-23</td>\n",
       "      <td>49840008.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>0312288115</td>\n",
       "      <td>1.152260e+09</td>\n",
       "      <td>CONFESSIONS OF A SOCIOPATHIC SOCIAL CLIMBER</td>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>114</td>\n",
       "      <td>Harold Lamb</td>\n",
       "      <td>Lamb, Harold, 1892-1962</td>\n",
       "      <td>n50037864</td>\n",
       "      <td>51824770</td>\n",
       "      <td>Q3270225</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1947-04-13</td>\n",
       "      <td>1161514.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.096085e+06</td>\n",
       "      <td>A GARDEN TO THE EASTWARD</td>\n",
       "      <td>2</td>\n",
       "      <td>1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1140</td>\n",
       "      <td>Jackie Collins</td>\n",
       "      <td>Collins, Jackie</td>\n",
       "      <td>n79113048</td>\n",
       "      <td>100253982</td>\n",
       "      <td>Q1837775</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2014-02-23</td>\n",
       "      <td>857754109.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>9781250050939</td>\n",
       "      <td>4.925689e+09</td>\n",
       "      <td>CONFESSIONS OF A WILD CHILD</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id              author                  author_authorized_heading  \\\n",
       "150  1132        Ann Patchett                              Patchett, Ann   \n",
       "151  1133       Joseph Finder                            Finder, Joseph,   \n",
       "152  1134  Jonathan Kellerman                        Kellerman, Jonathan   \n",
       "153  1135         Meyer Levin                    Levin, Meyer, 1905-1981   \n",
       "154  1136          J. D. Robb                         Robb, J. D., 1950-   \n",
       "155  1137   John D. MacDonald  MacDonald, John D. (John Dann), 1916-1986   \n",
       "156  1138        Jack Higgins                       Higgins, Jack, 1929-   \n",
       "157  1139          Adèle Lang                                Lang, Adèle   \n",
       "158   114         Harold Lamb                    Lamb, Harold, 1892-1962   \n",
       "159  1140      Jackie Collins                            Collins, Jackie   \n",
       "\n",
       "    author_lccn author_viaf author_wikidata  best_rank  debut_rank  \\\n",
       "150   n91108359    54362733         Q433485          1           1   \n",
       "151   n82134077    36971667        Q2125259         15          15   \n",
       "152   n79116770     2559502        Q1349245          1           1   \n",
       "153   n79055883    27067610         Q966340          2          10   \n",
       "154   n95081025    43552467             NaN          1           1   \n",
       "155   n79045202     7393665         Q558425          3           9   \n",
       "156   n79059853    39378467         Q433708         15          15   \n",
       "157   n98880006    10699038             NaN         14          14   \n",
       "158   n50037864    51824770        Q3270225         13          13   \n",
       "159   n79113048   100253982        Q1837775         13          13   \n",
       "\n",
       "     first_week         oclc  oclc_eholdings  oclc_holdings      oclc_isbn  \\\n",
       "150  2016-10-02  932576291.0           287.0         3917.0  9780062491794   \n",
       "151  2005-05-08   57352767.0            98.0         2386.0  9780312319168   \n",
       "152  2008-04-13  166358565.0           426.0         3919.0  9780345465276   \n",
       "153  1956-11-18     288021.0           162.0         1442.0     0786703199   \n",
       "154  2014-03-09  849719027.0           414.0         3602.0  9780399164439   \n",
       "155  1977-04-24    2767566.0            98.0         1551.0     0397012039   \n",
       "156  1985-07-07   11650359.0           315.0         2384.0     0812830253   \n",
       "157  2002-06-23   49840008.0            19.0          572.0     0312288115   \n",
       "158  1947-04-13    1161514.0             1.0          178.0            NaN   \n",
       "159  2014-02-23  857754109.0            67.0         1318.0  9781250050939   \n",
       "\n",
       "         oclc_owi                                        title  total_weeks  \\\n",
       "150  2.840265e+09                                 COMMONWEALTH           18   \n",
       "151  2.011108e+06                                  COMPANY MAN            1   \n",
       "152  1.128836e+08                                   COMPULSION            5   \n",
       "153  5.817719e+07                                   COMPULSION           54   \n",
       "154  1.744466e+09                           CONCEALED IN DEATH            4   \n",
       "155  1.965416e+08                                  CONDOMINIUM           27   \n",
       "156  4.233915e+06                                 CONFESSIONAL            1   \n",
       "157  1.152260e+09  CONFESSIONS OF A SOCIOPATHIC SOCIAL CLIMBER            1   \n",
       "158  2.096085e+06                     A GARDEN TO THE EASTWARD            2   \n",
       "159  4.925689e+09                  CONFESSIONS OF A WILD CHILD            1   \n",
       "\n",
       "     year  \n",
       "150  2016  \n",
       "151  2005  \n",
       "152  2008  \n",
       "153  1956  \n",
       "154  2014  \n",
       "155  1977  \n",
       "156  1985  \n",
       "157  2002  \n",
       "158  1947  \n",
       "159  2014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}