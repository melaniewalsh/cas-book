{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to get biographical information for a group of authors by using their LCCN, having already enriched the data to have the LCCN in pervious scripts. You can use other identifiers if you have them reconciled for the author already including Wikidata Q Id, VIAF, etc.\n",
    "\n",
    "Getting started checklist:\n",
    "1. A CSV or TSV file with metadata, at minimum it needs to contain the LCCN for the author.\n",
    "2. Python, with Pandas, Requests module installed and internet connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first steps will be defining some variables we will be using, set these variables below based on your setup:\n",
    "\n",
    "\n",
    "`path_to_tsv` - the path to the TSV/CSV file you want to run it on\n",
    "\n",
    "`lccn_column_name` - the name of the column that contains the lccn\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call, if you want the script to run slower\n",
    "\n",
    "\n",
    "We also will load the modules we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/path/to/the_file.tsv\"\n",
    "path_to_tsv = \"/Users/m/Downloads/hathitrust_post45fiction_metadata.tsv\"\n",
    "lccn_column_name = 'author_lccn'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read in all records and for each one make a SPARQL query out to wikidata for information about that author and store it in the file. You can look for a large number of properties, there is no guarantee that the property will exist for a person (Qid) so we wrap it in the `OPTIONAL { }` brackets. You can find the list of possible properties for people here: https://prop-explorer.toolforge.org/ just as a test lets look for Instagram (P2003) handles for these authors. You could of course look for more substantial properties like nationality (P27) or education (P69) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wikidtat_info(d):\n",
    "\n",
    "    if lccn_column_name not in d:\n",
    "       print(\"No lccn_column_name field found!\")\n",
    "       return d\n",
    "    \n",
    "    if type(d[lccn_column_name]) != str:     \n",
    "        # doesn't have one   \n",
    "        return d\n",
    "\n",
    "    sparql = f\"\"\"\n",
    "        SELECT ?item ?itemLabel ?instagram\n",
    "        WHERE \n",
    "        {{\n",
    "    \n",
    "        ?item wdt:P244 \"{d[lccn_column_name]}\".  #P244 is the LCCN number property\n",
    "        OPTIONAL {{\n",
    "            ?item wdt:P2003 ?instagram .\n",
    "        }}\n",
    "        \n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}       \n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'query' : sparql\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "\n",
    "    # did we get any results\n",
    "    if len(data['results']['bindings']) > 0:\n",
    "\n",
    "      for result in data['results']['bindings']:\n",
    "        if 'instagram' in result:\n",
    "            d['instagram'] = result['instagram']['value']\n",
    "\n",
    "\n",
    "\n",
    "    # if we need to script to run slower we can configure it setting the  pause_between_req variable above \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to load the Pandas module and load the data we are using, you can adjust the `sep` argument to change what delimiter is being used (for example if you are using a CSV file, change it to \",\"). Once loaded we pass each record to the `add_classify()` function to kick off adding the data to the record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df = df.apply(lambda d: add_wikidtat_info(d),axis=1 )  \n",
    "\n",
    "\n",
    "# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\n",
    "# df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the block above but it breaks the CSV/TSV into multiple chunks and writes it out after each chunk, this allows for recovery from any errors such as as internet timeout or other problems that would cause you to loose all progress unless the script runs flawlessly, you would likely want to use this approch for larger datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_wikidtat_info(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
