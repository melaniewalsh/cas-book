{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library of Congress\n",
    "\n",
    "In this tutorial we will be using the Library of Congress id.loc.gov to search for more biblographic information if we have a ISBN or OCLC number. Of course if you already have a LCCN number you could use that but this tutorial assumes you have either the ISBN or OCLC. While there are millions of records in LC's system it does not not include every ISBN and OCLC number in the world.\n",
    "\n",
    "\n",
    "Getting started checklist:\n",
    "1. A CSV or TSV file with metadata, at minimum it needs to contain the ISBN or OCLC id.\n",
    "2. Python, with Pandas, Requests module installed and internet connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first steps will be defining some variables we will be using, set these variables below based on your setup:\n",
    "\n",
    "\n",
    "`path_to_tsv` - the path to the TSV/CSV file you want to run it on\n",
    "\n",
    "`id_column_name` - the name of the column that contains the oclc or isbn number\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call, if you want the script to run slower\n",
    "\n",
    "\n",
    "We also will load the modules we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/path/to/the_file.tsv\"\n",
    "\n",
    "id_column_name = 'oclc'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download each MARC record from HathiTrust and then extract the information we want. We will then try to reconcile the name of the author against the id.loc.gov data service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(d):\n",
    "\n",
    "\n",
    "    \n",
    "    d[id_column_name] = int(d[id_column_name])\n",
    "        \n",
    "    url = f\"https://id.loc.gov/resources/instances/identifier/{d[id_column_name]}\"\n",
    "    \n",
    "    # here we use the head request to just get the headers which will have the LC identfier for the ISBN/OCLC if found\n",
    "    r = requests.head(url, headers={'Accept': 'application/json', 'User-Agent': user_agent})\n",
    "    print(url)\n",
    "    if 'Location' in r.headers:\n",
    "        # this means we found a hit, the Location to the record\n",
    "        # it will look something like this https://id.loc.gov/resources/instances/#######\n",
    "        # and we can just add on https://id.loc.gov/resources/instances/######.bibframe.json\n",
    "        # to get the instance level data, this is data about the manfestation\n",
    "\n",
    "        r2 = requests.get(r.headers['Location'] + '.bibframe.json', headers={'Accept': 'application/json', 'User-Agent': user_agent})\n",
    "        try:\n",
    "            graph_data = r2.json()\n",
    "        except:\n",
    "            print(\"Error with json:\",r.headers['Location'] + '.bibframe.json')\n",
    "            return d\n",
    "\n",
    "        # we can pull out the LCCN of the book for example\n",
    "        for graph in graph_data:\n",
    "\n",
    "            if '@type' in graph:\n",
    "                if 'http://id.loc.gov/ontologies/bibframe/Lccn' in graph['@type']:\n",
    "\n",
    "                    d['instance_lccn'] = graph['http://www.w3.org/1999/02/22-rdf-syntax-ns#value'][0]['@value']\n",
    "\n",
    "        # there are a lot of other information at the instance level you could get\n",
    "        print()\n",
    "        # but lets jump to the Work level to get more info\n",
    "        for graph in graph_data:\n",
    "            # make sure it is the right URI, normalize the http \n",
    "            if graph['@id'].replace(\"https://\",'http://') == r.headers['Location'].replace(\"https://\",'http://'):\n",
    "                if 'http://id.loc.gov/ontologies/bibframe/instanceOf' in graph:\n",
    "                    work_uri = graph['http://id.loc.gov/ontologies/bibframe/instanceOf'][0]['@id']\n",
    "\n",
    "                    # and we can do the same and grab the work info\n",
    "                    \n",
    "                    r3 = requests.get(work_uri + '.bibframe.json', headers={'Accept': 'application/json', 'User-Agent': user_agent})\n",
    "                    work_graph = r3.json()\n",
    "                    # lets say we want to find the contributor \n",
    "                    for w in work_graph:\n",
    "                        if '@type' in w:\n",
    "                            if 'http://id.loc.gov/ontologies/bibframe/Contribution' in w['@type']:\n",
    "                                if 'http://id.loc.gov/ontologies/bibframe/agent' in w:\n",
    "                                    agent_id = w['http://id.loc.gov/ontologies/bibframe/agent'][0]['@id']\n",
    "                                    \n",
    "                                    if 'id.loc.gov' in agent_id:\n",
    "                                        # the URI to the agent, split off the LCCN and store it\n",
    "                                        agent_lccn = agent_id.split(\"/\")[-1]\n",
    "                                        d['author_lccn'] = agent_lccn\n",
    "\n",
    "                    # at this point you could also look for Subject headings and other relationships\n",
    "                                    \n",
    "   \n",
    "    # if we need to script to run slower we can configure it setting the  pause_between_req variable above \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to load the Pandas module and load the data we are using, you can adjust the `sep` argument to change what delimiter is being used (for example if you are using a CSV file, change it to \",\"). Once loaded we pass each record to the `lookup()` function to kick off adding the data to the record\n",
    "\n",
    "The below code breaks the CSV/TSV into multiple chunks and writes it out after each chunk, this allows for recovery from any errors such as as internet timeout or other problems that would cause you to loose all progress unless the script runs flawlessly, you would likely want to use this approch for larger datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: lookup(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
